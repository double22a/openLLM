# openLLM

## LLM
| Model | Institution | Paper | Github | Demo | 
| --- | --- | --- | --- |  --- |
| chatGPT | OpenAI |  |  |  |
| ChatGLM | THUDM |  | https://github.com/THUDM/ChatGLM-6B |  |
| LLaLM | Meta | [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1) | https://github.com/facebookresearch/llama |  |

## Multimodal LLM
| Model | Institution | Paper | Github | Demo | 
| --- | --- | --- | --- |  --- |
| GPT4 | OpenAI |  |  |  |
| LLaVa | University of Wisconsinâ€“Madison | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | https://github.com/haotian-liu/LLaVA | https://llava.hliu.cc/ |
| MiniGPT-4 |  | [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) | https://github.com/Vision-CAIR/MiniGPT-4 | https://huggingface.co/spaces/Vision-CAIR/minigpt4 |
| MM-REACT | Microsoft | [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381) | https://github.com/microsoft/MM-REACT | https://huggingface.co/spaces/microsoft-cognitive-service/mm-react |
| VisionLLM | OpenGVLab | [VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks](https://arxiv.org/abs/2305.11175) | https://github.com/OpenGVLab/VisionLLM |  |
| InternGPT  | OpenGVLab | [InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language](https://arxiv.org/abs/2305.05662) | https://github.com/OpenGVLab/InternGPT | https://igpt.opengvlab.com/ |
| VisualGLM-6B | THUDM |  | https://github.com/THUDM/VisualGLM-6B | https://huggingface.co/spaces/lykeven/visualglm-6b |
